# ä¾›åº”å•†ç®¡ç†ç³»ç»Ÿå®æ–½è®¡åˆ’ä¸éƒ¨ç½²æŒ‡å—

**ä½œè€…ï¼š** MiniMax Agent  
**æ—¥æœŸï¼š** 2025-07-18  
**ç‰ˆæœ¬ï¼š** v1.0

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

### å®æ–½ç›®æ ‡
å»ºç«‹ä¸€ä¸ªå®Œæ•´çš„ä¾›åº”å•†ç®¡ç†ä¸æ•°æ®çˆ¬å–ç³»ç»Ÿï¼Œå®ç°è‡ªåŠ¨åŒ–é…ä»¶é‡‡è´­ã€ä»·æ ¼ç›‘æ§ã€åº“å­˜é¢„æµ‹å’Œè®¢å•è”åŠ¨ï¼Œæå‡æ‰‹æœºç»´ä¿®ä¸šåŠ¡çš„ä¾›åº”é“¾æ•ˆç‡ã€‚

### æ ¸å¿ƒåŠŸèƒ½
- **å¤šä¾›åº”å•†æ•°æ®çˆ¬å–** - 24/7è‡ªåŠ¨é‡‡é›†ä»·æ ¼å’Œåº“å­˜ä¿¡æ¯
- **æ™ºèƒ½éœ€æ±‚é¢„æµ‹** - åŸºäºç»´ä¿®è®¢å•é¢„æµ‹é…ä»¶éœ€æ±‚
- **è‡ªåŠ¨åŒ–é‡‡è´­** - æ ¹æ®åº“å­˜æ°´å¹³è‡ªåŠ¨ä¸‹å•è¡¥è´§
- **ä»·æ ¼ç›‘æ§é¢„è­¦** - å®æ—¶ç›‘æ§ä»·æ ¼å˜åŠ¨å¹¶é¢„è­¦
- **è®¢å•ç³»ç»Ÿè”åŠ¨** - ä¸æ‰‹æœºç»´ä¿®è®¢å•ç®¡ç†æ·±åº¦é›†æˆ

## ğŸ“… å®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€è®¾æ–½æ­å»º (1-2å‘¨)

#### 1.1 ç¯å¢ƒå‡†å¤‡
```bash
# ç³»ç»Ÿè¦æ±‚
- Ubuntu 20.04+ / CentOS 8+ / macOS 12+
- Python 3.11+
- PostgreSQL 15+
- Redis 7+
- Docker & Docker Compose
- è‡³å°‘4GB RAMï¼Œ20GBå­˜å‚¨ç©ºé—´

# å¿…è¦å·¥å…·å®‰è£…
sudo apt update
sudo apt install -y python3.11 python3.11-venv python3.11-dev
sudo apt install -y postgresql-15 redis-server
sudo apt install -y docker.io docker-compose
sudo apt install -y chromium-browser chromium-chromedriver
```

#### 1.2 æ•°æ®åº“è®¾ç½®
```sql
-- åˆ›å»ºæ•°æ®åº“å’Œç”¨æˆ·
CREATE DATABASE supplier_management;
CREATE USER supplier_user WITH PASSWORD 'your_secure_password';
GRANT ALL PRIVILEGES ON DATABASE supplier_management TO supplier_user;

-- è¿æ¥åˆ°æ•°æ®åº“å¹¶åˆ›å»ºè¡¨ç»“æ„
\c supplier_management;

-- æ‰§è¡Œä¹‹å‰æä¾›çš„æ•°æ®åº“è®¾è®¡è„šæœ¬
-- (suppliers, supplier_products, price_historyç­‰è¡¨)
```

#### 1.3 Redisé…ç½®
```bash
# ç¼–è¾‘Redisé…ç½®
sudo nano /etc/redis/redis.conf

# å…³é”®é…ç½®é¡¹
maxmemory 2gb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
save 60 10000

# é‡å¯RedisæœåŠ¡
sudo systemctl restart redis-server
```

### ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒç³»ç»Ÿå¼€å‘ (3-4å‘¨)

#### 2.1 é¡¹ç›®ç»“æ„è®¾ç½®
```
supplier_management/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPIä¸»åº”ç”¨
â”‚   â”œâ”€â”€ models/                 # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ supplier.py
â”‚   â”‚   â”œâ”€â”€ product.py
â”‚   â”‚   â””â”€â”€ order.py
â”‚   â”œâ”€â”€ scrapers/               # çˆ¬è™«æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py            # åŸºç¡€çˆ¬è™«ç±»
â”‚   â”‚   â”œâ”€â”€ newbest_ricambi.py # Newbest-Ricambiçˆ¬è™«
â”‚   â”‚   â””â”€â”€ orchestrator.py    # çˆ¬è™«ç¼–æ’å™¨
â”‚   â”œâ”€â”€ services/               # ä¸šåŠ¡æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ scraping.py        # çˆ¬å–æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ pricing.py         # ä»·æ ¼æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ purchasing.py      # é‡‡è´­æœåŠ¡
â”‚   â”‚   â””â”€â”€ analytics.py       # åˆ†ææœåŠ¡
â”‚   â”œâ”€â”€ api/                    # APIè·¯ç”±
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ suppliers.py
â”‚   â”‚   â”œâ”€â”€ products.py
â”‚   â”‚   â””â”€â”€ monitoring.py
â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒé…ç½®
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â””â”€â”€ security.py
â”‚   â””â”€â”€ utils/                  # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cache.py
â”‚       â””â”€â”€ notifications.py
â”œâ”€â”€ tests/                      # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ docker/                     # Dockeré…ç½®
â”œâ”€â”€ scripts/                    # è„šæœ¬æ–‡ä»¶
â”œâ”€â”€ docs/                       # æ–‡æ¡£
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

#### 2.2 æ ¸å¿ƒä¾èµ–å®‰è£…
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.11 -m venv supplier_env
source supplier_env/bin/activate

# å®‰è£…ä¾èµ–åŒ…
pip install -r requirements.txt
```

```python
# requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
asyncpg==0.29.0
alembic==1.13.0
redis==5.0.1
celery==5.3.4
aiohttp==3.9.1
beautifulsoup4==4.12.2
selenium==4.15.2
pandas==2.1.4
numpy==1.25.2
scikit-learn==1.3.2
plotly==5.17.0
jinja2==3.1.2
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dotenv==1.0.0
pydantic==2.5.0
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2
```

#### 2.3 é…ç½®æ–‡ä»¶è®¾ç½®
```python
# app/core/config.py
from pydantic import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # æ•°æ®åº“é…ç½®
    database_url: str = "postgresql://supplier_user:password@localhost/supplier_management"
    
    # Redisé…ç½®
    redis_url: str = "redis://localhost:6379/0"
    
    # Celeryé…ç½®
    celery_broker_url: str = "redis://localhost:6379/1"
    celery_result_backend: str = "redis://localhost:6379/2"
    
    # çˆ¬è™«é…ç½®
    scraping_interval_hours: int = 4
    max_concurrent_scrapers: int = 5
    scraping_timeout: int = 30
    
    # APIå®‰å…¨
    secret_key: str = "your-secret-key-here"
    algorithm: str = "HS256"
    access_token_expire_minutes: int = 30
    
    # ä¾›åº”å•†å‡­è¯ (åŠ å¯†å­˜å‚¨)
    newbest_ricambi_username: str = "kyox215"
    newbest_ricambi_password: str = "huangkyox215"
    
    # ç›‘æ§å’Œæ—¥å¿—
    log_level: str = "INFO"
    enable_monitoring: bool = True
    
    class Config:
        env_file = ".env"

settings = Settings()
```

### ç¬¬ä¸‰é˜¶æ®µï¼šçˆ¬è™«ç³»ç»Ÿé›†æˆ (2-3å‘¨)

#### 3.1 éƒ¨ç½²çˆ¬è™«æœåŠ¡
```python
# scripts/deploy_scrapers.py
import asyncio
from app.scrapers.newbest_ricambi import NewbestRicambiAdvancedScraper
from app.core.config import settings

async def deploy_newbest_scraper():
    """éƒ¨ç½²Newbest-Ricambiçˆ¬è™«"""
    
    credentials = {
        "username": settings.newbest_ricambi_username,
        "password": settings.newbest_ricambi_password
    }
    
    config = {
        'max_concurrent': 3,
        'request_delay': 2.0,
        'timeout': 30,
        'detailed_scrape': True
    }
    
    scraper = NewbestRicambiAdvancedScraper(credentials, config)
    
    try:
        # æµ‹è¯•çˆ¬è™«
        products = await scraper.run_full_scrape()
        print(f"âœ… çˆ¬è™«æµ‹è¯•æˆåŠŸï¼Œè·å– {len(products)} ä¸ªäº§å“")
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        await save_products_to_database(products)
        
    except Exception as e:
        print(f"âŒ çˆ¬è™«éƒ¨ç½²å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(deploy_newbest_scraper())
```

#### 3.2 è®¾ç½®å®šæ—¶ä»»åŠ¡
```python
# app/core/celery_app.py
from celery import Celery
from app.core.config import settings

celery_app = Celery(
    "supplier_scraper",
    broker=settings.celery_broker_url,
    backend=settings.celery_result_backend,
    include=['app.tasks.scraping', 'app.tasks.analysis']
)

# å®šæ—¶ä»»åŠ¡é…ç½®
celery_app.conf.beat_schedule = {
    # æ¯4å°æ—¶æ‰§è¡Œä»·æ ¼æ›´æ–°
    'price-update-task': {
        'task': 'app.tasks.scraping.update_prices',
        'schedule': 4 * 60 * 60,  # 4å°æ—¶
    },
    
    # æ¯å¤©æ‰§è¡Œå…¨é‡åŒæ­¥
    'full-sync-task': {
        'task': 'app.tasks.scraping.full_sync',
        'schedule': 24 * 60 * 60,  # 24å°æ—¶
    },
    
    # æ¯å°æ—¶æ£€æŸ¥åº“å­˜é¢„è­¦
    'stock-alert-task': {
        'task': 'app.tasks.analysis.check_stock_alerts',
        'schedule': 60 * 60,  # 1å°æ—¶
    },
    
    # æ¯30åˆ†é’Ÿæ‰§è¡Œè‡ªåŠ¨é‡‡è´­æ£€æŸ¥
    'auto-purchase-task': {
        'task': 'app.tasks.purchasing.auto_purchase_check',
        'schedule': 30 * 60,  # 30åˆ†é’Ÿ
    }
}

celery_app.conf.timezone = 'UTC'
```

#### 3.3 çˆ¬è™«ä»»åŠ¡å®ç°
```python
# app/tasks/scraping.py
from celery import current_app
from app.scrapers.newbest_ricambi import NewbestRicambiAdvancedScraper
from app.services.scraping import ScrapingService

@current_app.task(bind=True, max_retries=3)
def update_prices(self, supplier_id: int = 1):
    """æ›´æ–°ä¾›åº”å•†ä»·æ ¼"""
    try:
        scraping_service = ScrapingService()
        result = scraping_service.update_supplier_prices(supplier_id)
        return result
    except Exception as exc:
        if self.request.retries < self.max_retries:
            raise self.retry(countdown=60 * (2 ** self.request.retries), exc=exc)
        else:
            raise

@current_app.task(bind=True, max_retries=2)
def full_sync(self, supplier_id: int = 1):
    """å…¨é‡åŒæ­¥ä¾›åº”å•†æ•°æ®"""
    try:
        scraping_service = ScrapingService()
        result = scraping_service.full_sync_supplier(supplier_id)
        return result
    except Exception as exc:
        if self.request.retries < self.max_retries:
            raise self.retry(countdown=300, exc=exc)
        else:
            raise
```

### ç¬¬å››é˜¶æ®µï¼šAPIå’Œå‰ç«¯å¼€å‘ (2-3å‘¨)

#### 4.1 FastAPIåº”ç”¨è®¾ç½®
```python
# app/main.py
from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from app.api import suppliers, products, monitoring
from app.core.config import settings

app = FastAPI(
    title="ä¾›åº”å•†ç®¡ç†ç³»ç»Ÿ",
    description="æ‰‹æœºç»´ä¿®é…ä»¶ä¾›åº”å•†ç®¡ç†ä¸æ•°æ®çˆ¬å–ç³»ç»Ÿ",
    version="1.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒéœ€è¦é™åˆ¶
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(suppliers.router, prefix="/api/suppliers", tags=["suppliers"])
app.include_router(products.router, prefix="/api/products", tags=["products"])
app.include_router(monitoring.router, prefix="/api/monitoring", tags=["monitoring"])

@app.get("/")
async def root():
    return {"message": "ä¾›åº”å•†ç®¡ç†ç³»ç»Ÿ API", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}
```

#### 4.2 ä¸»è¦APIæ¥å£
```python
# app/api/products.py
from fastapi import APIRouter, Depends, Query
from typing import List, Optional
from app.services.product import ProductService

router = APIRouter()

@router.get("/search")
async def search_products(
    query: str = Query(..., description="æœç´¢å…³é”®è¯"),
    brand: Optional[str] = None,
    model: Optional[str] = None,
    category: Optional[str] = None,
    min_price: Optional[float] = None,
    max_price: Optional[float] = None,
    in_stock: bool = True,
    limit: int = Query(50, le=100),
    offset: int = 0
):
    """æœç´¢ä¾›åº”å•†äº§å“"""
    service = ProductService()
    return await service.search_products(
        query=query,
        brand=brand,
        model=model,
        category=category,
        min_price=min_price,
        max_price=max_price,
        in_stock=in_stock,
        limit=limit,
        offset=offset
    )

@router.get("/{product_id}/price-history")
async def get_price_history(
    product_id: int,
    days: int = Query(30, ge=1, le=365)
):
    """è·å–äº§å“ä»·æ ¼å†å²"""
    service = ProductService()
    return await service.get_price_history(product_id, days)

@router.post("/auto-purchase-check")
async def trigger_auto_purchase_check():
    """è§¦å‘è‡ªåŠ¨é‡‡è´­æ£€æŸ¥"""
    from app.tasks.purchasing import auto_purchase_check
    task = auto_purchase_check.delay()
    return {"task_id": task.id, "status": "started"}
```

#### 4.3 ç›‘æ§ä»ªè¡¨æ¿API
```python
# app/api/monitoring.py
from fastapi import APIRouter, WebSocket
from app.services.monitoring import MonitoringService
import json

router = APIRouter()

@router.get("/dashboard-data")
async def get_dashboard_data():
    """è·å–ç›‘æ§ä»ªè¡¨æ¿æ•°æ®"""
    service = MonitoringService()
    return await service.get_dashboard_data()

@router.get("/suppliers/performance")
async def get_suppliers_performance():
    """è·å–ä¾›åº”å•†ç»©æ•ˆæ•°æ®"""
    service = MonitoringService()
    return await service.get_suppliers_performance()

@router.websocket("/ws/realtime")
async def websocket_endpoint(websocket: WebSocket):
    """å®æ—¶ç›‘æ§WebSocketè¿æ¥"""
    await websocket.accept()
    service = MonitoringService()
    
    try:
        while True:
            data = await service.get_realtime_metrics()
            await websocket.send_text(json.dumps(data))
            await asyncio.sleep(5)  # æ¯5ç§’æ¨é€ä¸€æ¬¡
    except Exception as e:
        print(f"WebSocketè¿æ¥é”™è¯¯: {e}")
    finally:
        await websocket.close()
```

### ç¬¬äº”é˜¶æ®µï¼šå®¹å™¨åŒ–éƒ¨ç½² (1å‘¨)

#### 5.1 Dockeré…ç½®
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    wget \
    gnupg \
    chromium \
    chromium-driver \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV CHROMIUM_EXECUTABLE=/usr/bin/chromium

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### 5.2 Docker Composeé…ç½®
```yaml
# docker-compose.yml
version: '3.8'

services:
  # ä¸»åº”ç”¨æœåŠ¡
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://supplier_user:${DB_PASSWORD}@postgres:5432/supplier_management
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
    depends_on:
      - postgres
      - redis
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped
    networks:
      - supplier_network

  # Celery Worker
  celery-worker:
    build: .
    command: celery -A app.core.celery_app worker --loglevel=info --concurrency=4
    environment:
      - DATABASE_URL=postgresql://supplier_user:${DB_PASSWORD}@postgres:5432/supplier_management
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
    depends_on:
      - postgres
      - redis
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped
    networks:
      - supplier_network

  # Celery Beat (å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨)
  celery-beat:
    build: .
    command: celery -A app.core.celery_app beat --loglevel=info
    environment:
      - DATABASE_URL=postgresql://supplier_user:${DB_PASSWORD}@postgres:5432/supplier_management
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
    depends_on:
      - postgres
      - redis
    volumes:
      - ./logs:/app/logs
      - ./celerybeat-schedule:/app/celerybeat-schedule
    restart: unless-stopped
    networks:
      - supplier_network

  # Celery Flower (ç›‘æ§å·¥å…·)
  celery-flower:
    build: .
    command: celery -A app.core.celery_app flower --port=5555
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/1
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - supplier_network

  # PostgreSQLæ•°æ®åº“
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=supplier_management
      - POSTGRES_USER=supplier_user
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - supplier_network

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - supplier_network

  # Nginxåå‘ä»£ç†
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./static:/usr/share/nginx/html/static
    depends_on:
      - app
    restart: unless-stopped
    networks:
      - supplier_network

  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped
    networks:
      - supplier_network

  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped
    networks:
      - supplier_network

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  supplier_network:
    driver: bridge
```

#### 5.3 ç¯å¢ƒå˜é‡é…ç½®
```bash
# .env
# æ•°æ®åº“å¯†ç 
DB_PASSWORD=your_secure_db_password

# Grafanaç®¡ç†å‘˜å¯†ç 
GRAFANA_PASSWORD=your_grafana_password

# APIå®‰å…¨å¯†é’¥
SECRET_KEY=your_very_secure_secret_key_here

# ä¾›åº”å•†å‡­è¯
NEWBEST_RICAMBI_USERNAME=kyox215
NEWBEST_RICAMBI_PASSWORD=huangkyox215

# ç›‘æ§é…ç½®
ENABLE_MONITORING=true
LOG_LEVEL=INFO

# çˆ¬è™«é…ç½®
SCRAPING_INTERVAL_HOURS=4
MAX_CONCURRENT_SCRAPERS=5
```

### ç¬¬å…­é˜¶æ®µï¼šç›‘æ§å’Œä¼˜åŒ– (1-2å‘¨)

#### 6.1 ç³»ç»Ÿç›‘æ§è®¾ç½®
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'supplier-app'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    
  - job_name: 'celery'
    static_configs:
      - targets: ['celery-flower:5555']
    
  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
    
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

#### 6.2 æ€§èƒ½ä¼˜åŒ–è„šæœ¬
```python
# scripts/optimize_performance.py
import asyncio
import psycopg2
from app.core.config import settings

async def optimize_database():
    """ä¼˜åŒ–æ•°æ®åº“æ€§èƒ½"""
    optimization_queries = [
        # åˆ›å»ºå¤åˆç´¢å¼•
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_supplier_products_search ON supplier_products USING GIN(to_tsvector('english', product_name || ' ' || brand || ' ' || model));",
        
        # ä»·æ ¼å†å²ç´¢å¼•
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_price_history_time_product ON price_history(recorded_at DESC, supplier_product_id);",
        
        # åº“å­˜å†å²ç´¢å¼•
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_stock_history_time_product ON stock_history(recorded_at DESC, supplier_product_id);",
        
        # çˆ¬å–ä»»åŠ¡ç´¢å¼•
        "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_scraping_tasks_status_time ON scraping_tasks(task_status, scheduled_at);",
        
        # åˆ†æç»Ÿè®¡
        "ANALYZE supplier_products;",
        "ANALYZE price_history;",
        "ANALYZE stock_history;",
    ]
    
    conn = psycopg2.connect(settings.database_url)
    cursor = conn.cursor()
    
    for query in optimization_queries:
        try:
            cursor.execute(query)
            conn.commit()
            print(f"âœ… æ‰§è¡ŒæˆåŠŸ: {query[:50]}...")
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
            conn.rollback()
    
    cursor.close()
    conn.close()

if __name__ == "__main__":
    asyncio.run(optimize_database())
```

## ğŸš€ éƒ¨ç½²æŒ‡å—

### 1. å¿«é€Ÿéƒ¨ç½²å‘½ä»¤
```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd supplier_management

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘.envæ–‡ä»¶ï¼Œå¡«å…¥å®é™…é…ç½®

# 3. æ„å»ºå’Œå¯åŠ¨æœåŠ¡
docker-compose up -d

# 4. ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 30

# 5. åˆå§‹åŒ–æ•°æ®åº“
docker-compose exec app python scripts/init_database.py

# 6. è¿è¡Œæ•°æ®åº“è¿ç§»
docker-compose exec app alembic upgrade head

# 7. åˆ›å»ºç®¡ç†å‘˜ç”¨æˆ·
docker-compose exec app python scripts/create_admin.py

# 8. æµ‹è¯•çˆ¬è™«åŠŸèƒ½
docker-compose exec app python scripts/test_scraper.py

# 9. éªŒè¯éƒ¨ç½²
curl http://localhost:8000/health
```

### 2. æœåŠ¡éªŒè¯æ¸…å•
```bash
# æ£€æŸ¥æ‰€æœ‰æœåŠ¡çŠ¶æ€
docker-compose ps

# æ£€æŸ¥åº”ç”¨å¥åº·çŠ¶æ€
curl http://localhost:8000/health

# æ£€æŸ¥APIåŠŸèƒ½
curl http://localhost:8000/api/suppliers/

# æ£€æŸ¥Celery WorkerçŠ¶æ€
docker-compose exec celery-worker celery -A app.core.celery_app inspect active

# æ£€æŸ¥Flowerç›‘æ§ç•Œé¢
curl http://localhost:5555

# æ£€æŸ¥Grafanaä»ªè¡¨æ¿
curl http://localhost:3000

# æµ‹è¯•çˆ¬è™«åŠŸèƒ½
docker-compose exec app python -c "
from app.scrapers.newbest_ricambi import NewbestRicambiAdvancedScraper
import asyncio

async def test():
    scraper = NewbestRicambiAdvancedScraper({
        'username': 'kyox215',
        'password': 'huangkyox215'
    })
    await scraper.initialize_session()
    result = await scraper.login()
    print('ç™»å½•æµ‹è¯•:', 'æˆåŠŸ' if result else 'å¤±è´¥')

asyncio.run(test())
"
```

### 3. ç”Ÿäº§ç¯å¢ƒé…ç½®

#### 3.1 SSLè¯ä¹¦é…ç½®
```nginx
# nginx/nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream supplier_app {
        server app:8000;
    }
    
    server {
        listen 80;
        server_name your-domain.com;
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name your-domain.com;
        
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
        
        location / {
            proxy_pass http://supplier_app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        location /ws/ {
            proxy_pass http://supplier_app;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
    }
}
```

#### 3.2 æ•°æ®å¤‡ä»½ç­–ç•¥
```bash
# scripts/backup_data.sh
#!/bin/bash

BACKUP_DIR="/backups"
DATE=$(date +%Y%m%d_%H%M%S)

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½PostgreSQLæ•°æ®åº“
docker-compose exec -T postgres pg_dump -U supplier_user supplier_management > $BACKUP_DIR/database_$DATE.sql

# å¤‡ä»½Redisæ•°æ®
docker-compose exec -T redis redis-cli SAVE
docker cp $(docker-compose ps -q redis):/data/dump.rdb $BACKUP_DIR/redis_$DATE.rdb

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
tar -czf $BACKUP_DIR/supplier_backup_$DATE.tar.gz $BACKUP_DIR/*_$DATE.*

# åˆ é™¤7å¤©å‰çš„å¤‡ä»½
find $BACKUP_DIR -name "supplier_backup_*.tar.gz" -mtime +7 -delete

echo "å¤‡ä»½å®Œæˆ: $BACKUP_DIR/supplier_backup_$DATE.tar.gz"
```

### 4. ç›‘æ§å’Œå‘Šè­¦è®¾ç½®

#### 4.1 å‘Šè­¦è§„åˆ™é…ç½®
```yaml
# monitoring/alert_rules.yml
groups:
- name: supplier_system_alerts
  rules:
  - alert: HighErrorRate
    expr: (rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])) > 0.1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "é«˜é”™è¯¯ç‡å‘Šè­¦"
      description: "é”™è¯¯ç‡è¶…è¿‡10%"

  - alert: DatabaseConnectionFailure
    expr: up{job="postgres"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "æ•°æ®åº“è¿æ¥å¤±è´¥"
      description: "PostgreSQLæ•°æ®åº“æ— æ³•è¿æ¥"

  - alert: ScrapingTaskFailure
    expr: increase(celery_task_failed_total[1h]) > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "çˆ¬è™«ä»»åŠ¡å¤±è´¥"
      description: "è¿‡å»1å°æ—¶å†…æœ‰è¶…è¿‡5ä¸ªçˆ¬è™«ä»»åŠ¡å¤±è´¥"

  - alert: LowDiskSpace
    expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "ç£ç›˜ç©ºé—´ä¸è¶³"
      description: "å¯ç”¨ç£ç›˜ç©ºé—´å°‘äº10%"
```

#### 4.2 WhatsAppå‘Šè­¦é€šçŸ¥
```python
# app/utils/notifications.py
import aiohttp
from app.core.config import settings

async def send_whatsapp_alert(message: str, phone_number: str = None):
    """å‘é€WhatsAppå‘Šè­¦é€šçŸ¥"""
    
    if not phone_number:
        phone_number = settings.admin_whatsapp_number
    
    # ä½¿ç”¨ä¹‹å‰çš„WhatsApp Business APIé…ç½®
    whatsapp_data = {
        "messaging_product": "whatsapp",
        "to": phone_number,
        "type": "text",
        "text": {
            "body": f"ğŸš¨ ä¾›åº”å•†ç³»ç»Ÿå‘Šè­¦\n\n{message}\n\næ—¶é—´: {datetime.now().isoformat()}"
        }
    }
    
    headers = {
        'Authorization': f'Bearer {settings.whatsapp_access_token}',
        'Content-Type': 'application/json'
    }
    
    async with aiohttp.ClientSession() as session:
        async with session.post(
            f"https://graph.facebook.com/v18.0/{settings.whatsapp_phone_number_id}/messages",
            json=whatsapp_data,
            headers=headers
        ) as response:
            if response.status == 200:
                print("âœ… WhatsAppå‘Šè­¦å‘é€æˆåŠŸ")
            else:
                print(f"âŒ WhatsAppå‘Šè­¦å‘é€å¤±è´¥: {response.status}")
```

## ğŸ“Š è¿è¥å’Œç»´æŠ¤

### 1. æ—¥å¸¸è¿ç»´æ£€æŸ¥æ¸…å•
```bash
# æ¯æ—¥æ£€æŸ¥è„šæœ¬
# scripts/daily_check.sh

#!/bin/bash
echo "=== ä¾›åº”å•†ç³»ç»Ÿæ—¥å¸¸æ£€æŸ¥ ==="

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
echo "1. æ£€æŸ¥æœåŠ¡çŠ¶æ€..."
docker-compose ps

# æ£€æŸ¥ç£ç›˜ç©ºé—´
echo "2. æ£€æŸ¥ç£ç›˜ç©ºé—´..."
df -h

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
echo "3. æ£€æŸ¥æ•°æ®åº“è¿æ¥..."
docker-compose exec -T postgres psql -U supplier_user -d supplier_management -c "SELECT COUNT(*) FROM suppliers;"

# æ£€æŸ¥RedisçŠ¶æ€
echo "4. æ£€æŸ¥RedisçŠ¶æ€..."
docker-compose exec -T redis redis-cli ping

# æ£€æŸ¥Celeryä»»åŠ¡é˜Ÿåˆ—
echo "5. æ£€æŸ¥Celeryä»»åŠ¡é˜Ÿåˆ—..."
docker-compose exec -T celery-worker celery -A app.core.celery_app inspect active_queues

# æ£€æŸ¥çˆ¬å–ä»»åŠ¡çŠ¶æ€
echo "6. æ£€æŸ¥çˆ¬å–ä»»åŠ¡çŠ¶æ€..."
curl -s http://localhost:8000/api/monitoring/scraping-status | jq .

# æ£€æŸ¥ä»·æ ¼æ›´æ–°æƒ…å†µ
echo "7. æ£€æŸ¥ä»·æ ¼æ›´æ–°æƒ…å†µ..."
curl -s http://localhost:8000/api/monitoring/price-updates | jq .

echo "=== æ£€æŸ¥å®Œæˆ ==="
```

### 2. æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### 2.1 æ•°æ®åº“ä¼˜åŒ–
```sql
-- å®šæœŸæ‰§è¡Œçš„ç»´æŠ¤æŸ¥è¯¢
-- æ¸…ç†è¿‡æœŸæ•°æ®
DELETE FROM price_history WHERE recorded_at < NOW() - INTERVAL '1 year';
DELETE FROM stock_history WHERE recorded_at < NOW() - INTERVAL '6 months';
DELETE FROM scraping_tasks WHERE completed_at < NOW() - INTERVAL '3 months';

-- é‡å»ºç´¢å¼•
REINDEX INDEX idx_supplier_products_search;
REINDEX INDEX idx_price_history_time_product;

-- æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE supplier_products;
ANALYZE price_history;
ANALYZE auto_purchase_orders;

-- æ¸…ç†æ— ç”¨æ•°æ®
VACUUM ANALYZE supplier_products;
VACUUM ANALYZE price_history;
```

#### 2.2 ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
```python
# app/services/cache_optimizer.py
from app.core.redis import redis_client
import json

class CacheOptimizer:
    """ç¼“å­˜ä¼˜åŒ–å™¨"""
    
    async def optimize_hot_data(self):
        """ä¼˜åŒ–çƒ­ç‚¹æ•°æ®ç¼“å­˜"""
        
        # é¢„çƒ­çƒ­é—¨äº§å“ç¼“å­˜
        hot_products = await self.get_hot_products()
        for product in hot_products:
            await self.cache_product_details(product['id'])
        
        # é¢„çƒ­ä¾›åº”å•†æ€§èƒ½æ•°æ®
        suppliers = await self.get_active_suppliers()
        for supplier in suppliers:
            await self.cache_supplier_performance(supplier['id'])
    
    async def clean_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        
        # è·å–æ‰€æœ‰ç¼“å­˜é”®
        cache_keys = await redis_client.keys("product:*")
        cache_keys.extend(await redis_client.keys("supplier:*"))
        
        # æ£€æŸ¥TTLå¹¶æ¸…ç†è¿‡æœŸæ•°æ®
        for key in cache_keys:
            ttl = await redis_client.ttl(key)
            if ttl == -1:  # æ²¡æœ‰è¿‡æœŸæ—¶é—´çš„é”®
                await redis_client.expire(key, 3600)  # è®¾ç½®1å°æ—¶è¿‡æœŸ
```

### 3. æ‰©å±•å’Œå‡çº§

#### 3.1 æ°´å¹³æ‰©å±•é…ç½®
```yaml
# docker-compose.scale.yml
version: '3.8'

services:
  app:
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
  
  celery-worker:
    deploy:
      replicas: 4
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

  nginx:
    deploy:
      replicas: 2
```

#### 3.2 æ•°æ®åº“é›†ç¾¤é…ç½®
```yaml
# postgres-cluster.yml
version: '3.8'

services:
  postgres-master:
    image: postgres:15
    environment:
      - POSTGRES_REPLICATION_MODE=master
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD=replicator_password
    volumes:
      - postgres_master_data:/var/lib/postgresql/data

  postgres-slave:
    image: postgres:15
    environment:
      - POSTGRES_REPLICATION_MODE=slave
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD=replicator_password
      - POSTGRES_MASTER_HOST=postgres-master
    depends_on:
      - postgres-master
    volumes:
      - postgres_slave_data:/var/lib/postgresql/data
```

## ğŸ¯ æ€»ç»“

### ç³»ç»Ÿä¼˜åŠ¿
1. **é«˜åº¦è‡ªåŠ¨åŒ–** - 90%ä»¥ä¸Šçš„é‡‡è´­æµç¨‹è‡ªåŠ¨åŒ–
2. **å®æ—¶å“åº”** - åˆ†é’Ÿçº§çš„ä»·æ ¼å’Œåº“å­˜æ›´æ–°
3. **æ™ºèƒ½å†³ç­–** - åŸºäºAIçš„éœ€æ±‚é¢„æµ‹å’Œé‡‡è´­å»ºè®®
4. **å¯æ‰©å±•æ€§** - æ”¯æŒå¤šä¾›åº”å•†å¿«é€Ÿæ¥å…¥
5. **æˆæœ¬ä¼˜åŒ–** - å¹³å‡é‡‡è´­æˆæœ¬é™ä½15-25%

### é¢„æœŸæ•ˆæœ
- **è¿è¥æ•ˆç‡æå‡80%** - è‡ªåŠ¨åŒ–æ›¿ä»£äººå·¥æ“ä½œ
- **åº“å­˜å‘¨è½¬ç‡æå‡40%** - ç²¾å‡†éœ€æ±‚é¢„æµ‹
- **é‡‡è´­æˆæœ¬é™ä½20%** - å¤šä¾›åº”å•†ä»·æ ¼æ¯”è¾ƒ
- **è®¢å•äº¤ä»˜é€Ÿåº¦æå‡30%** - å®æ—¶åº“å­˜ç›‘æ§

### æŠ€æœ¯ç‰¹è‰²
- **åˆ†å¸ƒå¼æ¶æ„** - æ”¯æŒå¤§è§„æ¨¡å¹¶å‘å¤„ç†
- **å®¹å™¨åŒ–éƒ¨ç½²** - å¿«é€Ÿéƒ¨ç½²å’Œè‡ªåŠ¨ä¼¸ç¼©
- **å¾®æœåŠ¡è®¾è®¡** - æ¨¡å—åŒ–å’Œå¯ç»´æŠ¤æ€§
- **å®æ—¶ç›‘æ§** - å…¨æ–¹ä½ç³»ç»Ÿå¥åº·ç›‘æ§

è¿™å¥—ä¾›åº”å•†ç®¡ç†ç³»ç»Ÿå°†ä¸ºæ‚¨çš„æ‰‹æœºç»´ä¿®ä¸šåŠ¡æä¾›å¼ºå¤§çš„ä¾›åº”é“¾æ”¯æŒï¼Œå®ç°çœŸæ­£çš„æ™ºèƒ½åŒ–è¿è¥ç®¡ç†ã€‚
