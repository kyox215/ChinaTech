#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Newbest-Ricambi ä¾›åº”å•†ç½‘ç«™çˆ¬è™«å®ç°
ä¸“ä¸º https://newbest-ricambi.com å®šåˆ¶çš„æ•°æ®çˆ¬å–è§£å†³æ–¹æ¡ˆ

ä½œè€…: MiniMax Agent
æ—¥æœŸ: 2025-07-18
"""

import asyncio
import aiohttp
import json
import re
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin, urlparse
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ProductInfo:
    """äº§å“ä¿¡æ¯æ•°æ®ç±»"""
    supplier_product_id: str
    supplier_product_code: str
    product_name: str
    brand: str
    model: str
    category: str
    subcategory: str
    condition_grade: str
    original_price: float
    current_price: float
    currency: str
    stock_quantity: int
    min_order_quantity: int
    product_images: List[str]
    product_url: str
    specifications: Dict
    description: str
    last_scraped: str
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return asdict(self)

class NewbestRicambiAdvancedScraper:
    """Newbest-Ricambi é«˜çº§çˆ¬è™«"""
    
    def __init__(self, credentials: Dict[str, str], config: Dict = None):
        self.base_url = "https://newbest-ricambi.com"
        self.username = credentials["username"]
        self.password = credentials["password"]
        self.session = None
        self.is_logged_in = False
        
        # é»˜è®¤é…ç½®
        self.config = {
            'max_concurrent': 5,
            'request_delay': 1.0,
            'timeout': 30,
            'retry_count': 3,
            'save_images': True,
            'detailed_scrape': True,
            **(config or {})
        }
        
        # HTTPè¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'products_scraped': 0,
            'categories_found': 0,
            'errors_count': 0,
            'start_time': None,
            'end_time': None
        }
        
    async def initialize_session(self):
        """åˆå§‹åŒ–HTTPä¼šè¯"""
        connector = aiohttp.TCPConnector(
            limit=self.config['max_concurrent'],
            limit_per_host=self.config['max_concurrent'],
            ttl_dns_cache=300,
            use_dns_cache=True,
            ssl=False
        )
        
        timeout = aiohttp.ClientTimeout(
            total=self.config['timeout'],
            connect=10,
            sock_read=30
        )
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=self.headers
        )
        
        logger.info("HTTPä¼šè¯å·²åˆå§‹åŒ–")
        
    async def login(self) -> bool:
        """ç™»å½•åˆ°Newbest-Ricambiç½‘ç«™"""
        try:
            logger.info("å¼€å§‹ç™»å½•æµç¨‹...")
            
            # 1. è®¿é—®ç™»å½•é¡µé¢
            login_url = f"{self.base_url}/user.php"
            async with self.session.get(login_url) as response:
                if response.status != 200:
                    logger.error(f"è®¿é—®ç™»å½•é¡µé¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    return False
                    
                login_html = await response.text()
                
            # 2. è§£æç™»å½•è¡¨å•
            soup = BeautifulSoup(login_html, 'html.parser')
            
            # æŸ¥æ‰¾ç™»å½•è¡¨å•
            login_form = soup.find('form', {'method': 'post'}) or soup.find('form')
            if not login_form:
                logger.error("æœªæ‰¾åˆ°ç™»å½•è¡¨å•")
                return False
            
            # 3. æ„å»ºç™»å½•æ•°æ®
            form_data = {
                'email': self.username,
                'password': self.password,
                'rememberme': '1'
            }
            
            # æŸ¥æ‰¾éšè—å­—æ®µ
            hidden_inputs = login_form.find_all('input', {'type': 'hidden'})
            for input_field in hidden_inputs:
                name = input_field.get('name')
                value = input_field.get('value', '')
                if name:
                    form_data[name] = value
            
            # 4. æäº¤ç™»å½•è¡¨å•
            async with self.session.post(login_url, data=form_data) as response:
                response_text = await response.text()
                
                # æ£€æŸ¥ç™»å½•æ˜¯å¦æˆåŠŸ
                success_indicators = [
                    "æˆ‘çš„ä¿¡æ¯", "My Account", "Dashboard", "logout", "kyox215",
                    "account", "profile", "è®¢å•", "orders"
                ]
                
                login_success = any(indicator.lower() in response_text.lower() 
                                  for indicator in success_indicators)
                
                if login_success:
                    logger.info("âœ… ç™»å½•æˆåŠŸ")
                    self.is_logged_in = True
                    return True
                else:
                    logger.error("âŒ ç™»å½•å¤±è´¥ï¼Œæ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç ")
                    # ä¿å­˜ç™»å½•å¤±è´¥é¡µé¢ç”¨äºè°ƒè¯•
                    with open('login_failed.html', 'w', encoding='utf-8') as f:
                        f.write(response_text)
                    return False
                    
        except Exception as e:
            logger.error(f"ç™»å½•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            return False
    
    async def get_categories(self) -> List[Dict]:
        """è·å–æ‰€æœ‰äº§å“åˆ†ç±»"""
        try:
            logger.info("æ­£åœ¨è·å–äº§å“åˆ†ç±»...")
            
            async with self.session.get(self.base_url) as response:
                html = await response.text()
                
            soup = BeautifulSoup(html, 'html.parser')
            categories = []
            
            # æŸ¥æ‰¾åˆ†ç±»é“¾æ¥çš„ä¸åŒé€‰æ‹©å™¨
            category_selectors = [
                'div.categories a[href*="category"]',
                'ul.category-menu a',
                'nav a[href*="category"]',
                '.sidebar a[href*="category"]',
                'a[href*="goods.php"]',
                'a[href*="category"]'
            ]
            
            for selector in category_selectors:
                category_links = soup.select(selector)
                if category_links:
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨æ‰¾åˆ°åˆ†ç±»: {selector}")
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†ç±»é“¾æ¥ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„é“¾æ¥
            if not category_links:
                all_links = soup.find_all('a', href=True)
                category_links = [link for link in all_links 
                                if any(keyword in link.get('href', '').lower() 
                                      for keyword in ['category', 'goods', 'repair', 'phone', 'tablet'])]
            
            seen_urls = set()
            
            for link in category_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if not href or not text or len(text) > 100:
                    continue
                
                # æ„å»ºå®Œæ•´URL
                if href.startswith('/'):
                    full_url = self.base_url + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    full_url = urljoin(self.base_url, href)
                
                # é¿å…é‡å¤
                if full_url in seen_urls:
                    continue
                seen_urls.add(full_url)
                
                # åˆ¤æ–­åˆ†ç±»å±‚çº§
                parent_elements = link.find_parents()
                level = len([p for p in parent_elements if p.name in ['ul', 'li', 'div']])
                
                categories.append({
                    'name': text,
                    'url': full_url,
                    'level': min(level, 3),  # æœ€å¤š3çº§
                    'priority': self.calculate_category_priority(text, href)
                })
            
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            categories.sort(key=lambda x: x['priority'], reverse=True)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_categories = []
            seen_names = set()
            
            for cat in categories:
                if cat['name'] not in seen_names and len(unique_categories) < 50:
                    unique_categories.append(cat)
                    seen_names.add(cat['name'])
            
            self.stats['categories_found'] = len(unique_categories)
            logger.info(f"å‘ç° {len(unique_categories)} ä¸ªäº§å“åˆ†ç±»")
            
            return unique_categories
            
        except Exception as e:
            logger.error(f"è·å–åˆ†ç±»å¤±è´¥: {e}")
            return []
    
    def calculate_category_priority(self, name: str, href: str) -> int:
        """è®¡ç®—åˆ†ç±»ä¼˜å…ˆçº§"""
        priority = 0
        
        # é«˜ä¼˜å…ˆçº§å…³é”®è¯
        high_priority_keywords = [
            'iphone', 'samsung', 'huawei', 'xiaomi', 'apple',
            'screen', 'display', 'battery', 'repair parts'
        ]
        
        # ä¸­ä¼˜å…ˆçº§å…³é”®è¯
        medium_priority_keywords = [
            'phone', 'mobile', 'smartphone', 'android', 'ios'
        ]
        
        name_lower = name.lower()
        href_lower = href.lower()
        
        for keyword in high_priority_keywords:
            if keyword in name_lower or keyword in href_lower:
                priority += 10
                
        for keyword in medium_priority_keywords:
            if keyword in name_lower or keyword in href_lower:
                priority += 5
        
        # æ ¹æ®åˆ†ç±»åç§°é•¿åº¦è°ƒæ•´ä¼˜å…ˆçº§
        if 5 <= len(name) <= 20:
            priority += 3
        
        return priority
    
    async def scrape_category_products(self, category_url: str, max_pages: int = 20) -> List[ProductInfo]:
        """çˆ¬å–æŒ‡å®šåˆ†ç±»çš„æ‰€æœ‰äº§å“"""
        try:
            logger.info(f"å¼€å§‹çˆ¬å–åˆ†ç±»: {category_url}")
            
            all_products = []
            page = 1
            consecutive_empty_pages = 0
            
            while page <= max_pages and consecutive_empty_pages < 3:
                logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
                
                # æ„å»ºåˆ†é¡µURL
                page_url = self.build_page_url(category_url, page)
                
                try:
                    async with self.session.get(page_url) as response:
                        if response.status != 200:
                            logger.warning(f"é¡µé¢ {page} è¿”å›çŠ¶æ€ç : {response.status}")
                            consecutive_empty_pages += 1
                            page += 1
                            continue
                            
                        html = await response.text()
                        
                    # è§£æäº§å“åˆ—è¡¨
                    page_products = await self.parse_products_from_page(html, page_url)
                    
                    if page_products:
                        all_products.extend(page_products)
                        consecutive_empty_pages = 0
                        logger.info(f"ç¬¬ {page} é¡µè·å–åˆ° {len(page_products)} ä¸ªäº§å“")
                    else:
                        consecutive_empty_pages += 1
                        logger.info(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°äº§å“")
                    
                    page += 1
                    
                    # å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    await asyncio.sleep(self.config['request_delay'])
                    
                except Exception as e:
                    logger.error(f"çˆ¬å–ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}")
                    consecutive_empty_pages += 1
                    page += 1
            
            logger.info(f"åˆ†ç±»çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_products)} ä¸ªäº§å“")
            return all_products
            
        except Exception as e:
            logger.error(f"çˆ¬å–åˆ†ç±»äº§å“å¤±è´¥: {e}")
            return []
    
    def build_page_url(self, base_url: str, page: int) -> str:
        """æ„å»ºåˆ†é¡µURL"""
        if '?' in base_url:
            separator = '&'
        else:
            separator = '?'
        
        # å°è¯•ä¸åŒçš„åˆ†é¡µå‚æ•°å
        page_params = ['page', 'p', 'pagenum', 'pg']
        
        for param in page_params:
            page_url = f"{base_url}{separator}{param}={page}"
            return page_url
        
        return base_url
    
    async def parse_products_from_page(self, html: str, page_url: str) -> List[ProductInfo]:
        """ä»é¡µé¢HTMLä¸­è§£æäº§å“ä¿¡æ¯"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            products = []
            
            # å¤šç§äº§å“å®¹å™¨é€‰æ‹©å™¨
            product_selectors = [
                'div.product-item',
                'div.goods-item',
                'tr.product-row',
                'div.item',
                '.product',
                'tr[id*="goods"]',
                'tr[class*="goods"]'
            ]
            
            product_containers = []
            for selector in product_selectors:
                containers = soup.select(selector)
                if containers:
                    product_containers = containers
                    logger.debug(f"ä½¿ç”¨é€‰æ‹©å™¨æ‰¾åˆ°äº§å“: {selector}")
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†å®¹å™¨ï¼Œå°è¯•æŸ¥æ‰¾è¡¨æ ¼è¡Œ
            if not product_containers:
                table_rows = soup.find_all('tr')
                # è¿‡æ»¤æ‰è¡¨å¤´å’Œæ— ç”¨è¡Œ
                product_containers = [row for row in table_rows 
                                    if len(row.find_all(['td', 'th'])) >= 3
                                    and any(keyword in row.get_text().lower() 
                                           for keyword in ['â‚¬', 'eur', 'price', 'stock', 'grade'])]
            
            logger.debug(f"æ‰¾åˆ° {len(product_containers)} ä¸ªäº§å“å®¹å™¨")
            
            for container in product_containers:
                try:
                    product = await self.parse_single_product(container, page_url)
                    if product:
                        products.append(product)
                        self.stats['products_scraped'] += 1
                except Exception as e:
                    logger.debug(f"è§£æå•ä¸ªäº§å“å¤±è´¥: {e}")
                    self.stats['errors_count'] += 1
            
            return products
            
        except Exception as e:
            logger.error(f"è§£æé¡µé¢äº§å“å¤±è´¥: {e}")
            return []
    
    async def parse_single_product(self, container, page_url: str) -> Optional[ProductInfo]:
        """è§£æå•ä¸ªäº§å“ä¿¡æ¯"""
        try:
            # æå–äº§å“é“¾æ¥
            product_link = container.find('a', href=True)
            if not product_link:
                return None
            
            product_url = product_link['href']
            if product_url.startswith('/'):
                product_url = self.base_url + product_url
            elif not product_url.startswith('http'):
                product_url = urljoin(self.base_url, product_url)
            
            # æå–åŸºæœ¬ä¿¡æ¯
            product_name = self.extract_product_name(container)
            if not product_name or len(product_name) < 3:
                return None
            
            # æå–ä»·æ ¼ä¿¡æ¯
            price_info = self.extract_price_info(container)
            
            # æå–åº“å­˜ä¿¡æ¯
            stock_info = self.extract_stock_info(container)
            
            # æå–å›¾ç‰‡
            images = self.extract_product_images(container)
            
            # æå–å“ç‰Œå’Œå‹å·
            brand, model = self.extract_brand_model(product_name)
            
            # æå–çŠ¶å†µç­‰çº§
            condition = self.extract_condition_grade(product_name, container)
            
            # ç”Ÿæˆäº§å“ID
            product_id = self.generate_product_id(product_url, product_name)
            
            # å¦‚æœå¯ç”¨è¯¦ç»†çˆ¬å–ï¼Œè·å–è¯¦ç»†ä¿¡æ¯
            detailed_info = {}
            if self.config['detailed_scrape']:
                detailed_info = await self.get_product_details(product_url)
            
            product = ProductInfo(
                supplier_product_id=product_id,
                supplier_product_code=detailed_info.get('product_code', product_id),
                product_name=product_name,
                brand=brand,
                model=model,
                category=detailed_info.get('category', 'Mobile Parts'),
                subcategory=detailed_info.get('subcategory', ''),
                condition_grade=condition,
                original_price=price_info['original_price'],
                current_price=price_info['current_price'],
                currency=price_info['currency'],
                stock_quantity=stock_info['quantity'],
                min_order_quantity=stock_info['min_order'],
                product_images=images,
                product_url=product_url,
                specifications=detailed_info.get('specifications', {}),
                description=detailed_info.get('description', ''),
                last_scraped=datetime.now().isoformat()
            )
            
            return product
            
        except Exception as e:
            logger.debug(f"è§£æäº§å“ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def extract_product_name(self, container) -> str:
        """æå–äº§å“åç§°"""
        # å¤šç§åç§°é€‰æ‹©å™¨
        name_selectors = [
            'h3', 'h4', 'h5',
            '.product-name', '.title', '.name',
            'a[title]', 'td:first-child a',
            'strong', 'b'
        ]
        
        for selector in name_selectors:
            elem = container.select_one(selector)
            if elem:
                name = elem.get('title') or elem.get_text(strip=True)
                if name and len(name) >= 3:
                    return name
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»é“¾æ¥æ–‡æœ¬è·å–
        link = container.find('a')
        if link:
            return link.get_text(strip=True)
        
        return ""
    
    def extract_price_info(self, container) -> Dict:
        """æå–ä»·æ ¼ä¿¡æ¯"""
        price_info = {
            'original_price': 0.0,
            'current_price': 0.0,
            'currency': 'EUR'
        }
        
        # æŸ¥æ‰¾ä»·æ ¼å…ƒç´ 
        price_selectors = [
            '.price', '.cost', '.amount',
            'span[class*="price"]', 'td[class*="price"]',
            'strong', 'b'
        ]
        
        prices_found = []
        
        for selector in price_selectors:
            price_elems = container.select(selector)
            for elem in price_elems:
                price_text = elem.get_text(strip=True)
                price = self.extract_numeric_price(price_text)
                if price > 0:
                    prices_found.append(price)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œåœ¨æ•´ä¸ªå®¹å™¨ä¸­æœç´¢ä»·æ ¼æ¨¡å¼
        if not prices_found:
            container_text = container.get_text()
            price_patterns = [
                r'(\d+[.,]\d+)\s*â‚¬',
                r'â‚¬\s*(\d+[.,]\d+)',
                r'(\d+[.,]\d+)\s*EUR',
                r'EUR\s*(\d+[.,]\d+)',
                r'(\d+[.,]\d+)'
            ]
            
            for pattern in price_patterns:
                matches = re.findall(pattern, container_text)
                for match in matches:
                    price = self.extract_numeric_price(match)
                    if price > 0:
                        prices_found.append(price)
                        break
                if prices_found:
                    break
        
        if prices_found:
            price_info['current_price'] = min(prices_found)  # ä½¿ç”¨æœ€å°ä»·æ ¼
            price_info['original_price'] = max(prices_found)  # ä½¿ç”¨æœ€å¤§ä»·æ ¼ä½œä¸ºåŸä»·
        
        return price_info
    
    def extract_numeric_price(self, price_text: str) -> float:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—ä»·æ ¼"""
        try:
            # ç§»é™¤éæ•°å­—å­—ç¬¦ï¼Œä¿ç•™æ•°å­—ã€å°æ•°ç‚¹å’Œé€—å·
            clean_text = re.sub(r'[^\d.,]', '', price_text)
            
            if not clean_text:
                return 0.0
            
            # å¤„ç†é€—å·ä½œä¸ºå°æ•°åˆ†éš”ç¬¦çš„æƒ…å†µ
            if ',' in clean_text and '.' in clean_text:
                # å¦‚æœåŒæ—¶æœ‰é€—å·å’Œç‚¹ï¼Œå‡è®¾æœ€åä¸€ä¸ªæ˜¯å°æ•°åˆ†éš”ç¬¦
                if clean_text.rindex(',') > clean_text.rindex('.'):
                    clean_text = clean_text.replace('.', '').replace(',', '.')
                else:
                    clean_text = clean_text.replace(',', '')
            elif ',' in clean_text:
                # åªæœ‰é€—å·ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºå°æ•°åˆ†éš”ç¬¦
                comma_pos = clean_text.rindex(',')
                if len(clean_text) - comma_pos <= 3:  # é€—å·åé¢2ä½æˆ–3ä½æ•°å­—
                    clean_text = clean_text.replace(',', '.')
                else:
                    clean_text = clean_text.replace(',', '')
            
            return float(clean_text)
            
        except (ValueError, AttributeError):
            return 0.0
    
    def extract_stock_info(self, container) -> Dict:
        """æå–åº“å­˜ä¿¡æ¯"""
        stock_info = {
            'quantity': 0,
            'min_order': 1,
            'is_available': False
        }
        
        container_text = container.get_text().lower()
        
        # æ£€æŸ¥åº“å­˜çŠ¶æ€
        if any(keyword in container_text for keyword in ['out of stock', 'sold out', 'ç¼ºè´§', 'æ— åº“å­˜']):
            stock_info['quantity'] = 0
            stock_info['is_available'] = False
        elif any(keyword in container_text for keyword in ['in stock', 'available', 'æœ‰åº“å­˜', 'ç°è´§']):
            stock_info['quantity'] = 10  # é»˜è®¤åº“å­˜
            stock_info['is_available'] = True
        else:
            # å°è¯•ä»è¾“å…¥æ¡†è·å–æœ€å¤§æ•°é‡
            quantity_input = container.find('input', {'name': 'quantity'})
            if quantity_input:
                max_qty = quantity_input.get('max', '0')
                try:
                    stock_info['quantity'] = int(max_qty) if max_qty.isdigit() else 5
                    stock_info['is_available'] = stock_info['quantity'] > 0
                except:
                    stock_info['quantity'] = 5
                    stock_info['is_available'] = True
            else:
                # é»˜è®¤æœ‰åº“å­˜
                stock_info['quantity'] = 5
                stock_info['is_available'] = True
        
        return stock_info
    
    def extract_product_images(self, container) -> List[str]:
        """æå–äº§å“å›¾ç‰‡"""
        images = []
        
        # æŸ¥æ‰¾å›¾ç‰‡å…ƒç´ 
        img_elements = container.find_all('img')
        
        for img in img_elements:
            src = img.get('src') or img.get('data-src') or img.get('data-original')
            if src:
                # æ„å»ºå®Œæ•´URL
                if src.startswith('/'):
                    src = self.base_url + src
                elif not src.startswith('http'):
                    src = urljoin(self.base_url, src)
                
                # è¿‡æ»¤æ‰æ— æ•ˆå›¾ç‰‡
                if self.is_valid_product_image(src):
                    images.append(src)
        
        return images
    
    def is_valid_product_image(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„äº§å“å›¾ç‰‡"""
        invalid_keywords = [
            'logo', 'icon', 'button', 'arrow', 'banner',
            'placeholder', 'loading', '1x1', 'spacer'
        ]
        
        url_lower = url.lower()
        return not any(keyword in url_lower for keyword in invalid_keywords)
    
    def extract_brand_model(self, product_name: str) -> Tuple[str, str]:
        """ä»äº§å“åç§°ä¸­æå–å“ç‰Œå’Œå‹å·"""
        # å·²çŸ¥å“ç‰Œåˆ—è¡¨
        brands = [
            'APPLE', 'SAMSUNG', 'HUAWEI', 'XIAOMI', 'OPPO', 'VIVO', 'ONEPLUS',
            'GOOGLE', 'LG', 'SONY', 'MOTOROLA', 'NOKIA', 'REALME', 'HONOR',
            'IPHONE', 'GALAXY', 'PIXEL', 'XPERIA'
        ]
        
        name_upper = product_name.upper()
        detected_brand = "Unknown"
        detected_model = "Unknown"
        
        # æ£€æµ‹å“ç‰Œ
        for brand in brands:
            if brand in name_upper:
                detected_brand = brand
                break
        
        # æå–å‹å·
        model_patterns = [
            r'IPHONE\s*(\d+\s*(?:PRO|PLUS|MINI|MAX)?)',
            r'GALAXY\s*([A-Z]\d+)',
            r'(\w+\s*\d+\w*)',
            r'([A-Z]+\d+[A-Z]*)',
        ]
        
        for pattern in model_patterns:
            match = re.search(pattern, name_upper)
            if match:
                detected_model = match.group(1).strip()
                break
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ¨¡å¼ï¼Œä½¿ç”¨å‰å‡ ä¸ªå•è¯
        if detected_model == "Unknown":
            words = product_name.split()[:3]
            detected_model = " ".join(words)
        
        return detected_brand, detected_model
    
    def extract_condition_grade(self, product_name: str, container) -> str:
        """æå–å•†å“çŠ¶å†µç­‰çº§"""
        text_to_check = (product_name + " " + container.get_text()).lower()
        
        condition_mapping = {
            'grade a': 'Grade A',
            'grade b': 'Grade B',
            'grade c': 'Grade C',
            'new in blister': 'New In Blister',
            'original bulk': 'Original Bulk',
            'used': 'Used',
            'refurbished': 'Refurbished',
            'new': 'New',
            'original': 'Original'
        }
        
        for keyword, condition in condition_mapping.items():
            if keyword in text_to_check:
                return condition
        
        return "Unknown"
    
    def generate_product_id(self, product_url: str, product_name: str) -> str:
        """ç”Ÿæˆäº§å“ID"""
        # å°è¯•ä»URLä¸­æå–ID
        url_patterns = [
            r'/(\d+)/?$',
            r'id=(\d+)',
            r'product[_-](\d+)',
            r'goods[_-](\d+)'
        ]
        
        for pattern in url_patterns:
            match = re.search(pattern, product_url)
            if match:
                return match.group(1)
        
        # å¦‚æœURLä¸­æ²¡æœ‰IDï¼Œä½¿ç”¨åç§°å’ŒURLçš„å“ˆå¸Œå€¼
        content = f"{product_name}_{product_url}".encode('utf-8')
        return hashlib.md5(content).hexdigest()[:12]
    
    async def get_product_details(self, product_url: str) -> Dict:
        """è·å–äº§å“è¯¦ç»†ä¿¡æ¯"""
        try:
            async with self.session.get(product_url) as response:
                if response.status != 200:
                    return {}
                
                html = await response.text()
            
            soup = BeautifulSoup(html, 'html.parser')
            details = {}
            
            # æå–äº§å“ç¼–å·
            code_patterns = [
                r'ECS\d+',
                r'Product\s*ID:\s*(\w+)',
                r'SKU:\s*(\w+)',
                r'Code:\s*(\w+)'
            ]
            
            page_text = soup.get_text()
            for pattern in code_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    details['product_code'] = match.group(1) if match.groups() else match.group(0)
                    break
            
            # æå–äº§å“æè¿°
            desc_selectors = [
                '.product-description', '.description', '.product-details',
                'div[class*="desc"]', 'p[class*="desc"]'
            ]
            
            for selector in desc_selectors:
                desc_elem = soup.select_one(selector)
                if desc_elem:
                    details['description'] = desc_elem.get_text(strip=True)
                    break
            
            # æå–è§„æ ¼ä¿¡æ¯
            specs = {}
            spec_tables = soup.find_all('table', class_=['specifications', 'specs', 'details'])
            for table in spec_tables:
                for row in table.find_all('tr'):
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)
                        if key and value:
                            specs[key] = value
            
            details['specifications'] = specs
            
            # æå–æ‰€æœ‰é«˜è´¨é‡å›¾ç‰‡
            images = []
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src')
                if src and self.is_valid_product_image(src):
                    if src.startswith('/'):
                        src = self.base_url + src
                    images.append(src)
            
            details['images'] = images
            
            await asyncio.sleep(0.5)  # è¯¦æƒ…é¡µå»¶è¿Ÿ
            
            return details
            
        except Exception as e:
            logger.debug(f"è·å–äº§å“è¯¦æƒ…å¤±è´¥ {product_url}: {e}")
            return {}
    
    async def run_full_scrape(self) -> List[ProductInfo]:
        """æ‰§è¡Œå®Œæ•´çš„çˆ¬å–ä»»åŠ¡"""
        self.stats['start_time'] = datetime.now()
        logger.info("ğŸ•·ï¸ å¼€å§‹Newbest-Ricambiå…¨ç«™çˆ¬å–ä»»åŠ¡")
        
        try:
            # 1. åˆå§‹åŒ–ä¼šè¯
            await self.initialize_session()
            
            # 2. ç™»å½•
            if not await self.login():
                raise Exception("ç™»å½•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­çˆ¬å–")
            
            # 3. è·å–åˆ†ç±»
            categories = await self.get_categories()
            if not categories:
                raise Exception("æœªæ‰¾åˆ°ä»»ä½•äº§å“åˆ†ç±»")
            
            # 4. çˆ¬å–æ¯ä¸ªåˆ†ç±»çš„äº§å“
            all_products = []
            
            for i, category in enumerate(categories[:10], 1):  # é™åˆ¶å‰10ä¸ªåˆ†ç±»
                logger.info(f"ğŸ·ï¸ [{i}/{len(categories[:10])}] æ­£åœ¨çˆ¬å–åˆ†ç±»: {category['name']}")
                
                try:
                    category_products = await self.scrape_category_products(category['url'])
                    
                    # ä¸ºäº§å“æ·»åŠ åˆ†ç±»ä¿¡æ¯
                    for product in category_products:
                        product.category = category['name']
                        product.subcategory = f"Level {category['level']}"
                    
                    all_products.extend(category_products)
                    logger.info(f"âœ… åˆ†ç±» '{category['name']}' å®Œæˆï¼Œè·å– {len(category_products)} ä¸ªäº§å“")
                    
                    # åˆ†ç±»é—´çš„å»¶è¿Ÿ
                    await asyncio.sleep(self.config['request_delay'] * 2)
                    
                except Exception as e:
                    logger.error(f"âŒ çˆ¬å–åˆ†ç±» '{category['name']}' å¤±è´¥: {e}")
                    self.stats['errors_count'] += 1
                    continue
            
            self.stats['end_time'] = datetime.now()
            duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
            
            logger.info(f"""
ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆï¼
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   - æ€»äº§å“æ•°: {len(all_products)}
   - åˆ†ç±»æ•°: {self.stats['categories_found']}
   - é”™è¯¯æ•°: {self.stats['errors_count']}
   - è€—æ—¶: {duration:.2f} ç§’
   - å¹³å‡é€Ÿåº¦: {len(all_products)/duration:.2f} äº§å“/ç§’
            """)
            
            return all_products
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}")
            return []
            
        finally:
            if self.session:
                await self.session.close()
    
    async def export_to_json(self, products: List[ProductInfo], filename: str = None):
        """å¯¼å‡ºäº§å“æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"newbest_ricambi_products_{timestamp}.json"
        
        products_data = [product.to_dict() for product in products]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'scraping_info': {
                    'website': 'https://newbest-ricambi.com',
                    'total_products': len(products),
                    'scraped_at': datetime.now().isoformat(),
                    'stats': self.stats
                },
                'products': products_data
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… äº§å“æ•°æ®å·²å¯¼å‡ºåˆ°: {filename}")
        return filename

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
async def main():
    """ä¸»å‡½æ•°"""
    
    # é…ç½®çˆ¬è™«
    credentials = {
        "username": "kyox215",
        "password": "huangkyox215"
    }
    
    config = {
        'max_concurrent': 3,
        'request_delay': 1.5,
        'timeout': 30,
        'detailed_scrape': True,
        'save_images': True
    }
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = NewbestRicambiAdvancedScraper(credentials, config)
    
    try:
        # æ‰§è¡Œçˆ¬å–
        products = await scraper.run_full_scrape()
        
        if products:
            # å¯¼å‡ºæ•°æ®
            filename = await scraper.export_to_json(products)
            
            # æ‰“å°ç¤ºä¾‹äº§å“
            logger.info("\nğŸ“‹ äº§å“ç¤ºä¾‹:")
            for i, product in enumerate(products[:3], 1):
                logger.info(f"""
äº§å“ {i}:
  - åç§°: {product.product_name}
  - å“ç‰Œ: {product.brand} {product.model}
  - ä»·æ ¼: {product.current_price} {product.currency}
  - çŠ¶å†µ: {product.condition_grade}
  - åº“å­˜: {product.stock_quantity}
  - URL: {product.product_url}
                """)
        else:
            logger.error("âŒ æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•äº§å“")
            
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    # è¿è¡Œçˆ¬è™«
    asyncio.run(main())
